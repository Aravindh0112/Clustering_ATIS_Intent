# -*- coding: utf-8 -*-
"""K-MEANS(ATIS)_PRISM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mO8ZqSdrOFtaSQNm1UKPpJCRkqf2ZDVS

***K MEANS CLUSTERING ON ATIS***
"""

pip freeze > requirements.txt

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

datas=pd.read_csv('atis_intents_train.csv')
datas

datas.columns=['Intent','Sentence']
datas.head(10)

x=datas['Intent'].unique().tolist()
print(x)
len(x)

len(datas)
datas.shape

import seaborn as sns


sns.set_style('whitegrid')
plt.figure(figsize=(8, 6))
ax = sns.countplot(x='Intent', data=datas)
ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha="right")
plt.tight_layout()

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords


REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
REMOVE_NUM = re.compile('[\d+]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    
 
 
    text = text.lower() 


    text = REPLACE_BY_SPACE_RE.sub(' ', text) 
    

    text = text.replace('x', '') 
    

    text = REMOVE_NUM.sub('', text)

    text = BAD_SYMBOLS_RE.sub('', text) 


    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
    
 
    text = ' '.join(word for word in text.split() if (len(word) >= 2 and len(word) <= 21))

 
    text = ' '.join([stemmer.stem(word) for word in text.split()])
    
    return text

datas["Sentence"] = datas["Sentence"].apply(clean_text)
#datas.head(20)

from sklearn.feature_extraction.text import TfidfVectorizer


vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1, 2), stop_words='english')
X_train_vc = vectorizer.fit_transform(datas["Sentence"])

pd.DataFrame(X_train_vc.toarray(), columns=vectorizer.get_feature_names()).head()

k_clusters =12
from sklearn.cluster import KMeans

score = []
for i in range(1,k_clusters + 1):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=5,random_state=0)
    kmeans.fit(X_train_vc)
    score.append(kmeans.inertia_)
plt.plot(range(1,k_clusters + 1 ),score)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
#plt.savefig('elbow2.png')
plt.show()

k_clusters =8

model = KMeans(n_clusters=k_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.000001, random_state=0)
model.fit(X_train_vc)

clusters = model.predict(X_train_vc)

datas["Cluster Label"] = clusters
datas.head(20)

from sklearn.decomposition import PCA

sklearn_pca = PCA(n_components = 2)
Y_sklearn = sklearn_pca.fit_transform(X_train_vc.toarray())
kmeans = KMeans(n_clusters=k_clusters, max_iter=600, algorithm = 'auto')
fitted = kmeans.fit(Y_sklearn)
prediction = kmeans.predict(Y_sklearn)

plt.figure(figsize=(14, 7))
plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=prediction, s=40, cmap='viridis', linewidths=5)

centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='black', s=200, alpha=0.6);

# Just print it to the screen
order_centroids = model.cluster_centers_.argsort()[:, ::-1]

terms = vectorizer.get_feature_names()
for i in range(k_clusters):
    top_ten_words = [terms[ind] for ind in order_centroids[i, :k_clusters]]
    print("Cluster ",i,":")
    print()
    print("****************")
    print()
    for ele in top_ten_words:
      print(ele)
      #print()
    print()

"""***PREDICTION WITH TEST DATA***"""

dfs=pd.read_csv('atis_intents_test.csv')
dfs.columns=['Intent','Sentence']
dfs.head(10)

dfs['Sentence']=dfs['Sentence'].apply(clean_text)
dfs.head(10)

from sklearn.feature_extraction.text import TfidfVectorizer


vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1, 2), stop_words='english')
X_test_vec= vectorizer.fit_transform(dfs["Sentence"])

pd.DataFrame(X_test_vec.toarray(), columns=vectorizer.get_feature_names()).head()

model.fit(X_test_vec)

clusters1 = model.predict(X_test_vec)

dfs["Cluster Label"] = clusters1
dfs.head(20)

from sklearn.decomposition import PCA

sklearn_pca = PCA(n_components = 2)
Y_sklearn = sklearn_pca.fit_transform(X_test_vec.toarray())
kmeans = KMeans(n_clusters=k_clusters, max_iter=600, algorithm = 'auto')
fitted = kmeans.fit(Y_sklearn)
prediction = kmeans.predict(Y_sklearn)

plt.figure(figsize=(14, 7))
plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=prediction, s=40, cmap='viridis', linewidths=5)

centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='black', s=200, alpha=0.6);

order_centroids = model.cluster_centers_.argsort()[:, ::-1]

terms = vectorizer.get_feature_names()
for i in range(k_clusters):
    top_ten_words = [terms[ind] for ind in order_centroids[i, :k_clusters]]
    print("Cluster ",i,":")
    print()
    print("****************")
    print()
    for ele in top_ten_words:
      print(ele)
      #print()
    print()